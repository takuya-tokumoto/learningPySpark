{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "LearningPySpark_Chapter06.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "unh1HXK2xx77",
        "outputId": "c6c7c5f6-da4a-4b48-edca-2662236f5e56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "# SparkContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.getOrCreate() \n",
        "spark = SparkSession.builder.appName(\"My App\").getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 8s (1,637 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2_crkPNyYQH",
        "outputId": "cdb42293-c404-4832-9929-ed75f1b23c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7qooX4vxvnE"
      },
      "source": [
        "# Introducing ML package of PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q2wVp9AxvnE"
      },
      "source": [
        "## Predict chances of infant survival with ML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWFEQDzixvnE"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QazH8nLIxvnE"
      },
      "source": [
        "First, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_8JOo8lxvnF"
      },
      "source": [
        "import pyspark.sql.types as typ\n",
        "\n",
        "labels = [\n",
        "    ('INFANT_ALIVE_AT_REPORT', typ.IntegerType()),\n",
        "    ('BIRTH_PLACE', typ.StringType()),\n",
        "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
        "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
        "    ('CIG_BEFORE', typ.IntegerType()),\n",
        "    ('CIG_1_TRI', typ.IntegerType()),\n",
        "    ('CIG_2_TRI', typ.IntegerType()),\n",
        "    ('CIG_3_TRI', typ.IntegerType()),\n",
        "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
        "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
        "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
        "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
        "    ('DIABETES_PRE', typ.IntegerType()),\n",
        "    ('DIABETES_GEST', typ.IntegerType()),\n",
        "    ('HYP_TENS_PRE', typ.IntegerType()),\n",
        "    ('HYP_TENS_GEST', typ.IntegerType()),\n",
        "    ('PREV_BIRTH_PRETERM', typ.IntegerType())\n",
        "]\n",
        "\n",
        "schema = typ.StructType([\n",
        "    typ.StructField(e[0], e[1], nullable = False) for e in labels # StructField の値に None の値を格納できるかどうか指定\n",
        "])\n",
        "\n",
        "births = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/data/births_transformed.csv.gz', \n",
        "                        header=True, \n",
        "                        schema=schema)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9vYyhtSxvnF"
      },
      "source": [
        "### Create transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "eg1ig4z3xvnF"
      },
      "source": [
        "import pyspark.ml.feature as ft\n",
        "\n",
        "births = births \\\n",
        "    .withColumn(       'BIRTH_PLACE_INT', \n",
        "                births['BIRTH_PLACE'] \\\n",
        "                    .cast(typ.IntegerType()))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkc1caV7xvnF"
      },
      "source": [
        "このようにして、最初の`Transformer`を作成することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "v5CLmvcHxvnF"
      },
      "source": [
        "encoder = ft.OneHotEncoder(\n",
        "    inputCol='BIRTH_PLACE_INT', \n",
        "    outputCol='BIRTH_PLACE_VEC')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuRg6nPexvnG"
      },
      "source": [
        "それでは、すべての機能をまとめた1つの列を作ってみましょう。 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fBLm39eqxvnG"
      },
      "source": [
        "featuresCreator = ft.VectorAssembler( # ベクトル化\n",
        "    inputCols=[\n",
        "        col[0] \n",
        "        for col \n",
        "        in labels[2:]] + \\\n",
        "    [encoder.getOutputCol()], \n",
        "    outputCol='features'\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1zhDpmUxvnG"
      },
      "source": [
        "### Create an estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQhE5m2BxvnG"
      },
      "source": [
        "この例では、（もう一度）ロジスティック回帰モデルを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "kIBcthXfxvnG"
      },
      "source": [
        "import pyspark.ml.classification as cl"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mJr2XsNxvnG"
      },
      "source": [
        "読み込んだ後は、モデルを作成しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BUFs-0ZAxvnG"
      },
      "source": [
        "logistic = cl.LogisticRegression(\n",
        "    maxIter=10, \n",
        "    regParam=0.01, \n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CztiEF-GxvnH"
      },
      "source": [
        "### Create a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfZnl8foxvnH"
      },
      "source": [
        "あとは、`Pipeline`を作成して、モデルをフィットさせるだけです。まず、パッケージから`Pipeline`を読み込みましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPPky9KwxvnH"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "        encoder, \n",
        "        featuresCreator, \n",
        "        logistic\n",
        "    ])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISLRzTEIxvnH"
      },
      "source": [
        "### Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MN8KaiuxvnH"
      },
      "source": [
        "便利なことに、DataFrame APIには、`.randomSplit(...)`メソッドがあります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARyXe4X7xvnI"
      },
      "source": [
        "births_train, births_test = births \\\n",
        "    .randomSplit([0.7, 0.3], seed=666)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV70t9kuxvnI"
      },
      "source": [
        "それでは、パイプラインを実行し、モデルを推定してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ajgBTge1-PR",
        "outputId": "62acabb2-18c9-475c-de93-7b985c341adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "births_train.columns"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['INFANT_ALIVE_AT_REPORT',\n",
              " 'BIRTH_PLACE',\n",
              " 'MOTHER_AGE_YEARS',\n",
              " 'FATHER_COMBINED_AGE',\n",
              " 'CIG_BEFORE',\n",
              " 'CIG_1_TRI',\n",
              " 'CIG_2_TRI',\n",
              " 'CIG_3_TRI',\n",
              " 'MOTHER_HEIGHT_IN',\n",
              " 'MOTHER_PRE_WEIGHT',\n",
              " 'MOTHER_DELIVERY_WEIGHT',\n",
              " 'MOTHER_WEIGHT_GAIN',\n",
              " 'DIABETES_PRE',\n",
              " 'DIABETES_GEST',\n",
              " 'HYP_TENS_PRE',\n",
              " 'HYP_TENS_GEST',\n",
              " 'PREV_BIRTH_PRETERM',\n",
              " 'BIRTH_PLACE_INT']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sPF300dxvnI"
      },
      "source": [
        "model = pipeline.fit(births_train)\n",
        "test_model = model.transform(births_test)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awmz7DqMxvnI"
      },
      "source": [
        "これが`test_model`の姿です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkEZSmQdxvnI",
        "outputId": "4f9625c7-6d71-4999-99a8-03720acb626c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_model.take(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.0573, -1.0573]), probability=DenseVector([0.7422, 0.2578]), prediction=0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwqVkHNoxvnJ"
      },
      "source": [
        "### Model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PBUOD2jxvnJ"
      },
      "source": [
        "モデル精度検証"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQeM3puxvnJ",
        "outputId": "a260d114-fbfd-419a-a507-0e818a35ba41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pyspark.ml.evaluation as ev\n",
        "\n",
        "evaluator = ev.BinaryClassificationEvaluator(\n",
        "    rawPredictionCol='probability', \n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "\n",
        "print(evaluator.evaluate(test_model, \n",
        "     {evaluator.metricName: 'areaUnderROC'}))\n",
        "print(evaluator.evaluate(test_model, {evaluator.metricName: 'areaUnderPR'}))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7401301847095617\n",
            "0.7139354342365674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxeC4brxvnJ"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EZbLen5xvnJ"
      },
      "source": [
        "PySparkでは、`Pipeline`の定義を保存して後で使用することができます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ8YVwoa3aCO",
        "outputId": "aa0f7554-8c58-410c-c31e-a862f07179c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t\t\t\tspark-2.3.1-bin-hadoop2.7\n",
            "infant_oneHotEncoder_Logistic_Pipeline\tspark-2.3.1-bin-hadoop2.7.tgz\n",
            "sample_data\t\t\t\tspark-warehouse\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YrZb1FxxvnJ"
      },
      "source": [
        "pipelinePath = './infant_oneHotEncoder_Logistic_Pipeline'\n",
        "pipeline.write().overwrite().save(pipelinePath)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x76PKadaxvnJ"
      },
      "source": [
        "そのため、後でロードして、すぐに `.fit(...)` や予測に使用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KG75d9wxvnK",
        "outputId": "1bdaae90-f7bb-42dc-ed40-5bb6b5dea858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loadedPipeline = Pipeline.load(pipelinePath)\n",
        "loadedPipeline \\\n",
        "    .fit(births_train)\\\n",
        "    .transform(births_test)\\\n",
        "    .take(1)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(INFANT_ALIVE_AT_REPORT=0, BIRTH_PLACE='1', MOTHER_AGE_YEARS=13, FATHER_COMBINED_AGE=99, CIG_BEFORE=0, CIG_1_TRI=0, CIG_2_TRI=0, CIG_3_TRI=0, MOTHER_HEIGHT_IN=66, MOTHER_PRE_WEIGHT=133, MOTHER_DELIVERY_WEIGHT=135, MOTHER_WEIGHT_GAIN=2, DIABETES_PRE=0, DIABETES_GEST=0, HYP_TENS_PRE=0, HYP_TENS_GEST=0, PREV_BIRTH_PRETERM=0, BIRTH_PLACE_INT=1, BIRTH_PLACE_VEC=SparseVector(9, {1: 1.0}), features=SparseVector(24, {0: 13.0, 1: 99.0, 6: 66.0, 7: 133.0, 8: 135.0, 9: 2.0, 16: 1.0}), rawPrediction=DenseVector([1.0573, -1.0573]), probability=DenseVector([0.7422, 0.2578]), prediction=0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obLZpR42xvnK"
      },
      "source": [
        "モデル全体を保存することも可能"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0eKR0nKOxvnK"
      },
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "modelPath = './infant_oneHotEncoder_Logistic_PipelineModel'\n",
        "model.write().overwrite().save(modelPath)\n",
        "\n",
        "loadedPipelineModel = PipelineModel.load(modelPath)\n",
        "test_loadedModel = loadedPipelineModel.transform(births_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6UslFkBxvnK"
      },
      "source": [
        "## Parameter hyper-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWYK1bAxvnK"
      },
      "source": [
        "### Grid search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMSBagjlxvnK"
      },
      "source": [
        "パッケージの`.tune`部分を読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ooULMTbsxvnK"
      },
      "source": [
        "import pyspark.ml.tuning as tune"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-YdHcqpxvnK"
      },
      "source": [
        "次に、モデルとループさせたいパラメータのリストを指定します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nBeWKZMnxvnL"
      },
      "source": [
        "logistic = cl.LogisticRegression(\n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "\n",
        "grid = tune.ParamGridBuilder() \\\n",
        "    .addGrid(logistic.maxIter,  \n",
        "             [2, 10, 50]) \\\n",
        "    .addGrid(logistic.regParam, \n",
        "             [0.01, 0.05, 0.3]) \\\n",
        "    .build()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwCZWRjXxvnL"
      },
      "source": [
        "次に、モデルを比較する方法が必要です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "k3hWq5avxvnL"
      },
      "source": [
        "evaluator = ev.BinaryClassificationEvaluator(\n",
        "    rawPredictionCol='probability', \n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB7YsBZ4xvnL"
      },
      "source": [
        "検証作業を代行するロジックを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FddBK6X6xvnL"
      },
      "source": [
        "cv = tune.CrossValidator(\n",
        "    estimator=logistic, \n",
        "    estimatorParamMaps=grid, \n",
        "    evaluator=evaluator\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgTGUmYAxvnL"
      },
      "source": [
        "変換のみ行う`Pipeline`を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqcBTj7ixvnL"
      },
      "source": [
        "pipeline = Pipeline(stages=[encoder,featuresCreator])\n",
        "data_transformer = pipeline.fit(births_train)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZCD-NAsxvnM"
      },
      "source": [
        "このようにして、モデルに最適なパラメータの組み合わせを見つけることができました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKgjrdDrxvnM"
      },
      "source": [
        "cvModel = cv.fit(data_transformer.transform(births_train))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJVQKpJxvnM"
      },
      "source": [
        "`cvModel`は，推定された最良のモデルを返します．このモデルを使って，以前のモデルよりも良い結果が得られたかどうかを確認することができます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnZnwLMHxvnM",
        "outputId": "94a493a9-f210-4476-f5f4-6d6d9cf54b36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_train = data_transformer \\\n",
        "    .transform(births_test)\n",
        "results = cvModel.transform(data_train)\n",
        "\n",
        "print(evaluator.evaluate(results, \n",
        "     {evaluator.metricName: 'areaUnderROC'}))\n",
        "print(evaluator.evaluate(results, \n",
        "     {evaluator.metricName: 'areaUnderPR'}))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7404959803309813\n",
            "0.7157971108486731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKdxTAiXxvnM"
      },
      "source": [
        "どのパラメータが最も優れたモデルなのか？答えは少し複雑ですが、ここではそれを抽出する方法をご紹介します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpd7trTfxvnM",
        "outputId": "22211c7b-a2c0-458c-9409-2419b0b5b34b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results = [\n",
        "    (\n",
        "        [\n",
        "            {key.name: paramValue} \n",
        "            for key, paramValue \n",
        "            in zip(\n",
        "                params.keys(), \n",
        "                params.values())\n",
        "        ], metric\n",
        "    ) \n",
        "    for params, metric \n",
        "    in zip(\n",
        "        cvModel.getEstimatorParamMaps(), \n",
        "        cvModel.avgMetrics\n",
        "    )\n",
        "]\n",
        "\n",
        "sorted(results, \n",
        "       key=lambda el: el[1], \n",
        "       reverse=True)[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'maxIter': 50}, {'regParam': 0.01}], 0.7387129297187243)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qkzkIj7xvnM"
      },
      "source": [
        "### Train-Validation splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMbdoSdxvnM"
      },
      "source": [
        "`ChiSqSelector`を使用して、上位5つの特徴のみを選択し、モデルの複雑さを制限します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sn2eKECxvnN"
      },
      "source": [
        "selector = ft.ChiSqSelector(\n",
        "    numTopFeatures=5, \n",
        "    featuresCol=featuresCreator.getOutputCol(), \n",
        "    outputCol='selectedFeatures',\n",
        "    labelCol='INFANT_ALIVE_AT_REPORT'\n",
        ")\n",
        "\n",
        "logistic = cl.LogisticRegression(\n",
        "    labelCol='INFANT_ALIVE_AT_REPORT',\n",
        "    featuresCol='selectedFeatures'\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[encoder,featuresCreator,selector])\n",
        "data_transformer = pipeline.fit(births_train)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWc55c2ZxvnN"
      },
      "source": [
        "`TrainValidationSplit`オブジェクトは、`CrossValidator`モデルと同じ方法で作成されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Tx_5EAKmxvnN"
      },
      "source": [
        "tvs = tune.TrainValidationSplit(\n",
        "    estimator=logistic, \n",
        "    estimatorParamMaps=grid, \n",
        "    evaluator=evaluator\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Uh6nDFxvnN"
      },
      "source": [
        "先ほどと同じように、データをモデルに当てはめて、結果を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9KkLhbMxvnO",
        "outputId": "0eeed638-4b51-4f19-f3f4-6e4bf8f5fc78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tvsModel = tvs.fit(\n",
        "    data_transformer \\\n",
        "        .transform(births_train)\n",
        ")\n",
        "\n",
        "data_train = data_transformer \\\n",
        "    .transform(births_test)\n",
        "results = tvsModel.transform(data_train)\n",
        "\n",
        "print(evaluator.evaluate(results, \n",
        "     {evaluator.metricName: 'areaUnderROC'}))\n",
        "print(evaluator.evaluate(results, \n",
        "     {evaluator.metricName: 'areaUnderPR'}))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7294296314442145\n",
            "0.703775950281647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c-93SXqxvnO"
      },
      "source": [
        "## Other features of PySpark ML in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS9bu3pTxvnO"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PWV9UfMxvnO"
      },
      "source": [
        "#### NLP related feature extractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQP6p4BqxvnO"
      },
      "source": [
        "Simple dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aZc31S1hxvnO"
      },
      "source": [
        "text_data = spark.createDataFrame([\n",
        "    ['''Machine learning can be applied to a wide variety \n",
        "        of data types, such as vectors, text, images, and \n",
        "        structured data. This API adopts the DataFrame from \n",
        "        Spark SQL in order to support a variety of data types.'''],\n",
        "    ['''DataFrame supports many basic and structured types; \n",
        "        see the Spark SQL datatype reference for a list of \n",
        "        supported types. In addition to the types listed in \n",
        "        the Spark SQL guide, DataFrame can use ML Vector types.'''],\n",
        "    ['''A DataFrame can be created either implicitly or \n",
        "        explicitly from a regular RDD. See the code examples \n",
        "        below and the Spark SQL programming guide for examples.'''],\n",
        "    ['''Columns in a DataFrame are named. The code examples \n",
        "        below use names such as \"text,\" \"features,\" and \"label.\"''']\n",
        "], ['input'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-CFGC29xvnO"
      },
      "source": [
        "トークン化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "i2x5h-WRxvnP"
      },
      "source": [
        "tokenizer = ft.RegexTokenizer(\n",
        "    inputCol='input', \n",
        "    outputCol='input_arr', \n",
        "    pattern='\\s+|[,.\\\"]')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIP6_NVtxvnP"
      },
      "source": [
        "トークナイザーの出力は以下のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC5T2y-mxvnP",
        "outputId": "32559fdb-3e4f-48cc-e123-096c5eb35b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tok = tokenizer \\\n",
        "    .transform(text_data) \\\n",
        "    .select('input_arr') \n",
        "\n",
        "tok.take(1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(input_arr=['machine', 'learning', 'can', 'be', 'applied', 'to', 'a', 'wide', 'variety', 'of', 'data', 'types', 'such', 'as', 'vectors', 'text', 'images', 'and', 'structured', 'data', 'this', 'api', 'adopts', 'the', 'dataframe', 'from', 'spark', 'sql', 'in', 'order', 'to', 'support', 'a', 'variety', 'of', 'data', 'types'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdgcGZBHxvnP"
      },
      "source": [
        "`StopWordsRemover(...)`を利用。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RDdEMIykxvnP"
      },
      "source": [
        "stopwords = ft.StopWordsRemover(\n",
        "    inputCol=tokenizer.getOutputCol(), \n",
        "    outputCol='input_stop')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1clveqRvxvnP"
      },
      "source": [
        "このメソッドの出力は次のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V5dfXHtxvnP",
        "outputId": "d8e6fae2-931e-4924-8a99-617a9de105df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stopwords.transform(tok).select('input_stop').take(1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(input_stop=['machine', 'learning', 'applied', 'wide', 'variety', 'data', 'types', 'vectors', 'text', 'images', 'structured', 'data', 'api', 'adopts', 'dataframe', 'spark', 'sql', 'order', 'support', 'variety', 'data', 'types'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRN0IZ80xvnP"
      },
      "source": [
        "`NGram` modelと`Pipeline`の作成。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cEVev-DexvnP"
      },
      "source": [
        "ngram = ft.NGram(n=2, \n",
        "    inputCol=stopwords.getOutputCol(), \n",
        "    outputCol=\"nGrams\")\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPj5-3s4xvnQ"
      },
      "source": [
        "`pipeline`ができたところで、先ほどと同じような手順で作業を進めます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm41yBX3xvnQ",
        "outputId": "49807a5b-1e74-4964-a862-da1ff72cb75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_ngram = pipeline \\\n",
        "    .fit(text_data) \\\n",
        "    .transform(text_data)\n",
        "    \n",
        "data_ngram.select('nGrams').take(1)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(nGrams=['machine learning', 'learning applied', 'applied wide', 'wide variety', 'variety data', 'data types', 'types vectors', 'vectors text', 'text images', 'images structured', 'structured data', 'data api', 'api adopts', 'adopts dataframe', 'dataframe spark', 'spark sql', 'sql order', 'order support', 'support variety', 'variety data', 'data types'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n85kj_TvxvnQ"
      },
      "source": [
        "これで終わりです。N-gramを得て、それをさらにNLPの処理に使うことができます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vWsajzZxvnQ"
      },
      "source": [
        "#### Discretize continuous variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDZ0h57xvnR"
      },
      "source": [
        "このような場合には、値を離散的なバケットに*バンド*することが有効です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uCtKCTGQxvnS"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.arange(0, 100)\n",
        "x = x / 100.0 * np.pi * 4\n",
        "y = x * np.sin(x / 1.764) + 20.1234\n",
        "\n",
        "schema = typ.StructType([\n",
        "    typ.StructField('continuous_var', \n",
        "                    typ.DoubleType(), \n",
        "                    False\n",
        "   )\n",
        "])\n",
        "\n",
        "data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt7mLFlKxvnT"
      },
      "source": [
        "\n",
        "`QuantileDiscretizer`モデルを使って、連続変数を5つのバケットに分割します（`numBuckets`パラメータを参照）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UXBVsqrMxvnT"
      },
      "source": [
        "discretizer = ft.QuantileDiscretizer(\n",
        "    numBuckets=5, \n",
        "    inputCol='continuous_var', \n",
        "    outputCol='discretized')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SkEGVt_xvnU"
      },
      "source": [
        "Let's see what we got."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQlG5yXMxvnU",
        "outputId": "78a058e0-f310-4357-f31b-17fd39916b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_discretized = discretizer.fit(data).transform(data)\n",
        "\n",
        "data_discretized \\\n",
        "    .groupby('discretized')\\\n",
        "    .mean('continuous_var')\\\n",
        "    .sort('discretized')\\\n",
        "    .collect()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(discretized=0.0, avg(continuous_var)=12.314360733007913),\n",
              " Row(discretized=1.0, avg(continuous_var)=16.04624479334747),\n",
              " Row(discretized=2.0, avg(continuous_var)=20.250799478352594),\n",
              " Row(discretized=3.0, avg(continuous_var)=22.040988218437327),\n",
              " Row(discretized=4.0, avg(continuous_var)=24.264824657002865)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEExERaxvnU"
      },
      "source": [
        "#### Standardizing continuous variables\n",
        "\n",
        "連続的な変数のベクトル表現を作成する（単一の浮動小数点数に過ぎないが)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "T-49CFE8xvnV"
      },
      "source": [
        "vectorizer = ft.VectorAssembler(\n",
        "    inputCols=['continuous_var'], \n",
        "    outputCol= 'continuous_vec')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyCYOmzHxvnV"
      },
      "source": [
        "Build a `normalizer` and a `pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhMoBUyRxvnV"
      },
      "source": [
        "normalizer = ft.StandardScaler(\n",
        "    inputCol=vectorizer.getOutputCol(), \n",
        "    outputCol='normalized', \n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[vectorizer, normalizer])\n",
        "data_standardized = pipeline.fit(data).transform(data)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2u16WPOxvnW"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-S_8qj_xvnW"
      },
      "source": [
        "それでは，`RandomForestClassfier`を使って，乳児の生存率をモデル化してみましょう． \n",
        "\n",
        "まず，ラベル機能を `DoubleType` にキャストする必要があります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3adgC1vHxvnX"
      },
      "source": [
        "import pyspark.sql.functions as func\n",
        "\n",
        "births = births.withColumn(\n",
        "    'INFANT_ALIVE_AT_REPORT', \n",
        "    func.col('INFANT_ALIVE_AT_REPORT').cast(typ.DoubleType())\n",
        ")\n",
        "\n",
        "births_train, births_test = births \\\n",
        "    .randomSplit([0.7, 0.3], seed=666)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myTnCLPrxvnY"
      },
      "source": [
        "We are ready to build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5qFLuIqxvnY"
      },
      "source": [
        "classifier = cl.RandomForestClassifier(\n",
        "    numTrees=5, \n",
        "    maxDepth=5, \n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages=[\n",
        "        encoder,\n",
        "        featuresCreator, \n",
        "        classifier])\n",
        "\n",
        "model = pipeline.fit(births_train)\n",
        "test = model.transform(births_test)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwU6HjpIxvnZ"
      },
      "source": [
        "それでは，`RandomForestClassifier`モデルが`LogisticRegression`と比較してどのような結果になるかを見てみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_lZKdstxvnZ",
        "outputId": "ea3ec585-c345-468c-8ca8-1816a2588017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluator = ev.BinaryClassificationEvaluator(\n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "print(evaluator.evaluate(test, \n",
        "    {evaluator.metricName: \"areaUnderROC\"}))\n",
        "print(evaluator.evaluate(test, \n",
        "    {evaluator.metricName: \"areaUnderPR\"}))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7672804407579924\n",
            "0.7257802869957367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4C4FJ-DnxvnZ"
      },
      "source": [
        "では、1本の木でどれだけの効果が得られるか試してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "athU1MItxvnZ",
        "outputId": "d41d5b92-7b48-4cc3-cdcc-dc5978d27d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier = cl.DecisionTreeClassifier(\n",
        "    maxDepth=5, \n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "pipeline = Pipeline(stages=[\n",
        "        encoder,\n",
        "        featuresCreator, \n",
        "        classifier]\n",
        ")\n",
        "\n",
        "model = pipeline.fit(births_train)\n",
        "test = model.transform(births_test)\n",
        "\n",
        "evaluator = ev.BinaryClassificationEvaluator(\n",
        "    labelCol='INFANT_ALIVE_AT_REPORT')\n",
        "print(evaluator.evaluate(test, \n",
        "     {evaluator.metricName: \"areaUnderROC\"}))\n",
        "print(evaluator.evaluate(test, \n",
        "     {evaluator.metricName: \"areaUnderPR\"}))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7574139108659715\n",
            "0.7414554196764107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve7se3gsxvna"
      },
      "source": [
        "### Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti6LRlb-xvna"
      },
      "source": [
        "この例では、k-meansモデルを使用して、出生データの類似性を見つけます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "KtLE68s0xvna"
      },
      "source": [
        "import pyspark.ml.clustering as clus\n",
        "\n",
        "kmeans = clus.KMeans(k = 5, \n",
        "    featuresCol='features')\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "        encoder,\n",
        "        featuresCreator, \n",
        "        kmeans]\n",
        ")\n",
        "\n",
        "model = pipeline.fit(births_train)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHvlyaoMxvna"
      },
      "source": [
        "モデルを推定した後、クラスター間の違いを見つけられるかどうか見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mxG4x4txvna",
        "outputId": "2f5fc13e-1f97-45a4-866b-70f165fa62b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test = model.transform(births_test)\n",
        "\n",
        "test \\\n",
        "    .groupBy('prediction') \\\n",
        "    .agg({\n",
        "        '*': 'count', \n",
        "        'MOTHER_HEIGHT_IN': 'avg'\n",
        "    }).collect()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(prediction=1, avg(MOTHER_HEIGHT_IN)=66.64658634538152, count(1)=249),\n",
              " Row(prediction=3, avg(MOTHER_HEIGHT_IN)=83.91154791154791, count(1)=407),\n",
              " Row(prediction=4, avg(MOTHER_HEIGHT_IN)=65.38934651290499, count(1)=3642),\n",
              " Row(prediction=2, avg(MOTHER_HEIGHT_IN)=67.69473684210526, count(1)=475),\n",
              " Row(prediction=0, avg(MOTHER_HEIGHT_IN)=63.90958873491283, count(1)=8948)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "i-0h4a0Fxvna"
      },
      "source": [
        "NLPの分野では、トピック抽出などの問題は、トピックが似ている文書を検出するクラスタリングに依存しています。まず、データセットを作ってみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VwzHuDhMxvnb"
      },
      "source": [
        "text_data = spark.createDataFrame([\n",
        "    ['''To make a computer do anything, you have to write a \n",
        "    computer program. To write a computer program, you have \n",
        "    to tell the computer, step by step, exactly what you want \n",
        "    it to do. The computer then \"executes\" the program, \n",
        "    following each step mechanically, to accomplish the end \n",
        "    goal. When you are telling the computer what to do, you \n",
        "    also get to choose how it's going to do it. That's where \n",
        "    computer algorithms come in. The algorithm is the basic \n",
        "    technique used to get the job done. Let's follow an \n",
        "    example to help get an understanding of the algorithm \n",
        "    concept.'''],\n",
        "    ['''Laptop computers use batteries to run while not \n",
        "    connected to mains. When we overcharge or overheat \n",
        "    lithium ion batteries, the materials inside start to \n",
        "    break down and produce bubbles of oxygen, carbon dioxide, \n",
        "    and other gases. Pressure builds up, and the hot battery \n",
        "    swells from a rectangle into a pillow shape. Sometimes \n",
        "    the phone involved will operate afterwards. Other times \n",
        "    it will die. And occasionally—kapow! To see what's \n",
        "    happening inside the battery when it swells, the CLS team \n",
        "    used an x-ray technology called computed tomography.'''],\n",
        "    ['''This technology describes a technique where touch \n",
        "    sensors can be placed around any side of a device \n",
        "    allowing for new input sources. The patent also notes \n",
        "    that physical buttons (such as the volume controls) could \n",
        "    be replaced by these embedded touch sensors. In essence \n",
        "    Apple could drop the current buttons and move towards \n",
        "    touch-enabled areas on the device for the existing UI. It \n",
        "    could also open up areas for new UI paradigms, such as \n",
        "    using the back of the smartphone for quick scrolling or \n",
        "    page turning.'''],\n",
        "    ['''The National Park Service is a proud protector of \n",
        "    America’s lands. Preserving our land not only safeguards \n",
        "    the natural environment, but it also protects the \n",
        "    stories, cultures, and histories of our ancestors. As we \n",
        "    face the increasingly dire consequences of climate \n",
        "    change, it is imperative that we continue to expand \n",
        "    America’s protected lands under the oversight of the \n",
        "    National Park Service. Doing so combats climate change \n",
        "    and allows all American’s to visit, explore, and learn \n",
        "    from these treasured places for generations to come. It \n",
        "    is critical that President Obama acts swiftly to preserve \n",
        "    land that is at risk of external threats before the end \n",
        "    of his term as it has become blatantly clear that the \n",
        "    next administration will not hold the same value for our \n",
        "    environment over the next four years.'''],\n",
        "    ['''The National Park Foundation, the official charitable \n",
        "    partner of the National Park Service, enriches America’s \n",
        "    national parks and programs through the support of \n",
        "    private citizens, park lovers, stewards of nature, \n",
        "    history enthusiasts, and wilderness adventurers. \n",
        "    Chartered by Congress in 1967, the Foundation grew out of \n",
        "    a legacy of park protection that began over a century \n",
        "    ago, when ordinary citizens took action to establish and \n",
        "    protect our national parks. Today, the National Park \n",
        "    Foundation carries on the tradition of early park \n",
        "    advocates, big thinkers, doers and dreamers—from John \n",
        "    Muir and Ansel Adams to President Theodore Roosevelt.'''],\n",
        "    ['''Australia has over 500 national parks. Over 28 \n",
        "    million hectares of land is designated as national \n",
        "    parkland, accounting for almost four per cent of \n",
        "    Australia's land areas. In addition, a further six per \n",
        "    cent of Australia is protected and includes state \n",
        "    forests, nature parks and conservation reserves.National \n",
        "    parks are usually large areas of land that are protected \n",
        "    because they have unspoilt landscapes and a diverse \n",
        "    number of native plants and animals. This means that \n",
        "    commercial activities such as farming are prohibited and \n",
        "    human activity is strictly monitored.''']\n",
        "], ['documents'])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3DJgxsOxvnb"
      },
      "source": [
        "まず、今回も`RegexTokenizer`と`StopWordsRemover`のモデルを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cAcIt7gjxvnb"
      },
      "source": [
        "tokenizer = ft.RegexTokenizer(\n",
        "    inputCol='documents', \n",
        "    outputCol='input_arr', \n",
        "    pattern='\\s+|[,.\\\"]')\n",
        "\n",
        "stopwords = ft.StopWordsRemover(\n",
        "    inputCol=tokenizer.getOutputCol(), \n",
        "    outputCol='input_stop')"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yPRBzENxvnb"
      },
      "source": [
        "`CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNaQ9qEDxvnc",
        "outputId": "a58b2847-40a9-48c1-b395-fda43fc1077e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stringIndexer = ft.CountVectorizer(\n",
        "    inputCol=stopwords.getOutputCol(), \n",
        "    outputCol=\"input_indexed\")\n",
        "\n",
        "tokenized = stopwords \\\n",
        "    .transform(\n",
        "        tokenizer\\\n",
        "            .transform(text_data)\n",
        "    )\n",
        "    \n",
        "stringIndexer \\\n",
        "    .fit(tokenized)\\\n",
        "    .transform(tokenized)\\\n",
        "    .select('input_indexed')\\\n",
        "    .take(2)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(input_indexed=SparseVector(257, {2: 7.0, 6: 1.0, 7: 3.0, 8: 3.0, 13: 3.0, 14: 1.0, 15: 2.0, 19: 1.0, 22: 2.0, 23: 1.0, 38: 1.0, 69: 1.0, 83: 1.0, 108: 1.0, 112: 1.0, 122: 1.0, 124: 1.0, 126: 1.0, 136: 1.0, 160: 1.0, 178: 1.0, 184: 1.0, 186: 1.0, 196: 1.0, 202: 1.0, 224: 1.0, 229: 1.0, 236: 1.0, 237: 1.0, 240: 1.0, 243: 1.0, 249: 1.0, 253: 1.0})),\n",
              " Row(input_indexed=SparseVector(257, {23: 1.0, 24: 2.0, 30: 2.0, 31: 1.0, 37: 2.0, 40: 2.0, 47: 1.0, 52: 1.0, 53: 1.0, 59: 1.0, 60: 1.0, 70: 1.0, 71: 1.0, 74: 1.0, 76: 1.0, 89: 1.0, 91: 1.0, 96: 1.0, 97: 1.0, 99: 1.0, 101: 1.0, 102: 1.0, 107: 1.0, 109: 1.0, 117: 1.0, 127: 1.0, 130: 1.0, 138: 1.0, 141: 1.0, 148: 1.0, 149: 1.0, 153: 1.0, 158: 1.0, 167: 1.0, 172: 1.0, 181: 1.0, 182: 1.0, 183: 1.0, 187: 1.0, 195: 1.0, 199: 1.0, 208: 1.0, 216: 1.0, 227: 1.0, 228: 1.0, 233: 1.0, 247: 1.0}))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niuc1_O6xvnc"
      },
      "source": [
        "トピックの抽出には、LDAモデル（Latent Dirichlet Allocationモデル）を使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0kPTbjuLxvnc"
      },
      "source": [
        "clustering = clus.LDA(k=2, optimizer='online', featuresCol=stringIndexer.getOutputCol())"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9W_7m3bxvnc"
      },
      "source": [
        "Put these puzzles together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qqy3hWaxvnc"
      },
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "        tokenizer, \n",
        "        stopwords,\n",
        "        stringIndexer, \n",
        "        clustering]\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCJPX9fxvnd"
      },
      "source": [
        "Let's see if we have properly uncovered the topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGcGP9n4xvnd",
        "outputId": "6ca52394-147d-490d-9a3f-a869a729ab2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "topics = pipeline \\\n",
        "    .fit(text_data) \\\n",
        "    .transform(text_data)\n",
        "\n",
        "topics.select('topicDistribution').collect()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(topicDistribution=DenseVector([0.0101, 0.9899])),\n",
              " Row(topicDistribution=DenseVector([0.6425, 0.3575])),\n",
              " Row(topicDistribution=DenseVector([0.9446, 0.0554])),\n",
              " Row(topicDistribution=DenseVector([0.9002, 0.0998])),\n",
              " Row(topicDistribution=DenseVector([0.9862, 0.0138])),\n",
              " Row(topicDistribution=DenseVector([0.9875, 0.0125]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY0q0Yx2xvnd"
      },
      "source": [
        "### Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13q01Jzsxvnd"
      },
      "source": [
        "このセクションでは、`MOTHER_WEIGHT_GAIN`の予測を試みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Yt-u5ca1xvnd"
      },
      "source": [
        "features = ['MOTHER_AGE_YEARS','MOTHER_HEIGHT_IN',\n",
        "            'MOTHER_PRE_WEIGHT','DIABETES_PRE',\n",
        "            'DIABETES_GEST','HYP_TENS_PRE', \n",
        "            'HYP_TENS_GEST', 'PREV_BIRTH_PRETERM',\n",
        "            'CIG_BEFORE','CIG_1_TRI', 'CIG_2_TRI', \n",
        "            'CIG_3_TRI'\n",
        "           ]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYDdAGQxvnd"
      },
      "source": [
        "まず、すべての機能を照合し、`ChiSqSelector`を使って、上位6つの重要な特徴量のみを選択します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ksv_Xl6axvnd"
      },
      "source": [
        "featuresCreator = ft.VectorAssembler(\n",
        "    inputCols=[col for col in features[1:]], \n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "selector = ft.ChiSqSelector(\n",
        "    numTopFeatures=6, \n",
        "    outputCol=\"selectedFeatures\", \n",
        "    labelCol='MOTHER_WEIGHT_GAIN'\n",
        ")"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_pjqLdcxvne"
      },
      "source": [
        "体重増加を予測するために、我々は勾配ブーストを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "SILp62UVxvne"
      },
      "source": [
        "import pyspark.ml.regression as reg\n",
        "\n",
        "regressor = reg.GBTRegressor(\n",
        "    maxIter=15, \n",
        "    maxDepth=3,\n",
        "    labelCol='MOTHER_WEIGHT_GAIN')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fNvwUAgxvne"
      },
      "source": [
        "Finally, again, we put it all together into a `Pipeline`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMZbJPHvxvne"
      },
      "source": [
        "pipeline = Pipeline(stages=[\n",
        "        featuresCreator, \n",
        "        selector,\n",
        "        regressor])\n",
        "\n",
        "weightGain = pipeline.fit(births_train)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx_tnovpxvne"
      },
      "source": [
        "Having created the `weightGain` model, let's see if it performs well on our testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u18WTUJ9xvne",
        "outputId": "97da995a-477b-49e9-8c7e-6f02e0dde066",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluator = ev.RegressionEvaluator(\n",
        "    predictionCol=\"prediction\", \n",
        "    labelCol='MOTHER_WEIGHT_GAIN')\n",
        "\n",
        "print(evaluator.evaluate(\n",
        "     weightGain.transform(births_test), \n",
        "    {evaluator.metricName: 'r2'}))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4889202506355995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBri87WtBw9y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}