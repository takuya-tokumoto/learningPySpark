{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "LearningPySpark_Chapter05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takuya-tokumoto/learningPySpark/blob/master/Chapter05/LearningPySpark_Chapter05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbZQJJmwTA1p",
        "outputId": "6612fa23-bdfd-4ebe-c6fd-a7ecdb9db3b8"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "# SparkContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.getOrCreate() \n",
        "spark = SparkSession.builder.appName(\"My App\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 5s (2,783 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83iUlScAEGrq"
      },
      "source": [
        "# Introducing MLib package of PySpark "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr-vqqLXEGrs"
      },
      "source": [
        "## Load and transform the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37K0d5NhEGrt"
      },
      "source": [
        "前章と同様に、まずデータセットのスキーマを指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8BKa2mvOEGru"
      },
      "source": [
        "import pyspark.sql.types as typ\n",
        "\n",
        "labels = [\n",
        "    ('INFANT_ALIVE_AT_REPORT', typ.StringType()),\n",
        "    ('BIRTH_YEAR', typ.IntegerType()),\n",
        "    ('BIRTH_MONTH', typ.IntegerType()),\n",
        "    ('BIRTH_PLACE', typ.StringType()),\n",
        "    ('MOTHER_AGE_YEARS', typ.IntegerType()),\n",
        "    ('MOTHER_RACE_6CODE', typ.StringType()),\n",
        "    ('MOTHER_EDUCATION', typ.StringType()),\n",
        "    ('FATHER_COMBINED_AGE', typ.IntegerType()),\n",
        "    ('FATHER_EDUCATION', typ.StringType()),\n",
        "    ('MONTH_PRECARE_RECODE', typ.StringType()),\n",
        "    ('CIG_BEFORE', typ.IntegerType()),\n",
        "    ('CIG_1_TRI', typ.IntegerType()),\n",
        "    ('CIG_2_TRI', typ.IntegerType()),\n",
        "    ('CIG_3_TRI', typ.IntegerType()),\n",
        "    ('MOTHER_HEIGHT_IN', typ.IntegerType()),\n",
        "    ('MOTHER_BMI_RECODE', typ.IntegerType()),\n",
        "    ('MOTHER_PRE_WEIGHT', typ.IntegerType()),\n",
        "    ('MOTHER_DELIVERY_WEIGHT', typ.IntegerType()),\n",
        "    ('MOTHER_WEIGHT_GAIN', typ.IntegerType()),\n",
        "    ('DIABETES_PRE', typ.StringType()),\n",
        "    ('DIABETES_GEST', typ.StringType()),\n",
        "    ('HYP_TENS_PRE', typ.StringType()),\n",
        "    ('HYP_TENS_GEST', typ.StringType()),\n",
        "    ('PREV_BIRTH_PRETERM', typ.StringType()),\n",
        "    ('NO_RISK', typ.StringType()),\n",
        "    ('NO_INFECTIONS_REPORTED', typ.StringType()),\n",
        "    ('LABOR_IND', typ.StringType()),\n",
        "    ('LABOR_AUGM', typ.StringType()),\n",
        "    ('STEROIDS', typ.StringType()),\n",
        "    ('ANTIBIOTICS', typ.StringType()),\n",
        "    ('ANESTHESIA', typ.StringType()),\n",
        "    ('DELIV_METHOD_RECODE_COMB', typ.StringType()),\n",
        "    ('ATTENDANT_BIRTH', typ.StringType()),\n",
        "    ('APGAR_5', typ.IntegerType()),\n",
        "    ('APGAR_5_RECODE', typ.StringType()),\n",
        "    ('APGAR_10', typ.IntegerType()),\n",
        "    ('APGAR_10_RECODE', typ.StringType()),\n",
        "    ('INFANT_SEX', typ.StringType()),\n",
        "    ('OBSTETRIC_GESTATION_WEEKS', typ.IntegerType()),\n",
        "    ('INFANT_WEIGHT_GRAMS', typ.IntegerType()),\n",
        "    ('INFANT_ASSIST_VENTI', typ.StringType()),\n",
        "    ('INFANT_ASSIST_VENTI_6HRS', typ.StringType()),\n",
        "    ('INFANT_NICU_ADMISSION', typ.StringType()),\n",
        "    ('INFANT_SURFACANT', typ.StringType()),\n",
        "    ('INFANT_ANTIBIOTICS', typ.StringType()),\n",
        "    ('INFANT_SEIZURES', typ.StringType()),\n",
        "    ('INFANT_NO_ABNORMALITIES', typ.StringType()),\n",
        "    ('INFANT_ANCEPHALY', typ.StringType()),\n",
        "    ('INFANT_MENINGOMYELOCELE', typ.StringType()),\n",
        "    ('INFANT_LIMB_REDUCTION', typ.StringType()),\n",
        "    ('INFANT_DOWN_SYNDROME', typ.StringType()),\n",
        "    ('INFANT_SUSPECTED_CHROMOSOMAL_DISORDER', typ.StringType()),\n",
        "    ('INFANT_NO_CONGENITAL_ANOMALIES_CHECKED', typ.StringType()),\n",
        "    ('INFANT_BREASTFED', typ.StringType())\n",
        "]\n",
        "\n",
        "schema = typ.StructType([\n",
        "        typ.StructField(e[0], e[1], False) for e in labels\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P7AhtbAEGrv"
      },
      "source": [
        "Next, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwxAtz7DXUCe",
        "outputId": "89a9061d-ecb9-45e1-8bf0-b492143b904f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcKAO9dhEGrv"
      },
      "source": [
        "births = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/data/births_train.csv.gz', \n",
        "                        header=True, \n",
        "                        schema=schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "FcjKzSudEGrw"
      },
      "source": [
        "辞書の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hBY3NvDXEGrx"
      },
      "source": [
        "recode_dictionary = {\n",
        "    'YNU': {\n",
        "        'Y': 1,\n",
        "        'N': 0,\n",
        "        'U': 0\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n4yoi4PEGry"
      },
      "source": [
        "私たちの目的は、「`INFANT_ALIVE_AT_REPORT`」が1か0かを予測することです。したがって、乳児に関連するすべての特徴を削除します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtjB4eV7EGrz"
      },
      "source": [
        "selected_features = [\n",
        "    'INFANT_ALIVE_AT_REPORT', \n",
        "    'BIRTH_PLACE', \n",
        "    'MOTHER_AGE_YEARS', \n",
        "    'FATHER_COMBINED_AGE', \n",
        "    'CIG_BEFORE', \n",
        "    'CIG_1_TRI', \n",
        "    'CIG_2_TRI', \n",
        "    'CIG_3_TRI', \n",
        "    'MOTHER_HEIGHT_IN', \n",
        "    'MOTHER_PRE_WEIGHT', \n",
        "    'MOTHER_DELIVERY_WEIGHT', \n",
        "    'MOTHER_WEIGHT_GAIN', \n",
        "    'DIABETES_PRE', \n",
        "    'DIABETES_GEST', \n",
        "    'HYP_TENS_PRE', \n",
        "    'HYP_TENS_GEST', \n",
        "    'PREV_BIRTH_PRETERM'\n",
        "]\n",
        "\n",
        "births_trimmed = births.select(selected_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvztAxAHEGrz"
      },
      "source": [
        "Specify the recoding methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zLUOIL9sEGrz"
      },
      "source": [
        "import pyspark.sql.functions as func\n",
        "\n",
        "def recode(col, key):        \n",
        "    return recode_dictionary[key][col] \n",
        "\n",
        "def correct_cig(feat):\n",
        "    return func \\\n",
        "        .when(func.col(feat) != 99, func.col(feat))\\\n",
        "        .otherwise(0)\n",
        "\n",
        "rec_integer = func.udf(recode, typ.IntegerType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoZJEO0HEGr0"
      },
      "source": [
        "吸ったタバコの本数に関する機能を修正します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QdBbwoXpEGr0"
      },
      "source": [
        "births_transformed = births_trimmed \\\n",
        "    .withColumn('CIG_BEFORE', correct_cig('CIG_BEFORE'))\\\n",
        "    .withColumn('CIG_1_TRI', correct_cig('CIG_1_TRI'))\\\n",
        "    .withColumn('CIG_2_TRI', correct_cig('CIG_2_TRI'))\\\n",
        "    .withColumn('CIG_3_TRI', correct_cig('CIG_3_TRI'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqGPJXsrEGr0"
      },
      "source": [
        "Yes/No/Unknownで集計。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zgdbsIeWEGr1"
      },
      "source": [
        "cols = [(col.name, col.dataType) for col in births_trimmed.schema]\n",
        "\n",
        "YNU_cols = []\n",
        "\n",
        "for i, s in enumerate(cols):\n",
        "    if s[1] == typ.StringType():\n",
        "        dis = births.select(s[0]) \\\n",
        "            .distinct() \\\n",
        "            .rdd \\\n",
        "            .map(lambda row: row[0]) \\\n",
        "            .collect()\n",
        "\n",
        "        if 'Y' in dis:\n",
        "            YNU_cols.append(s[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-In8TbXEGr1"
      },
      "source": [
        "DataFrames can transform the features *in bulk* while selecting features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z2D75WFEGr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750f2be6-c488-429f-a64e-1c71dffaadc8"
      },
      "source": [
        "births.select([\n",
        "        'INFANT_NICU_ADMISSION', \n",
        "        rec_integer(\n",
        "            'INFANT_NICU_ADMISSION', func.lit('YNU')\n",
        "        ) \\\n",
        "        .alias('INFANT_NICU_ADMISSION_RECODE')]\n",
        "     ).take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
              " Row(INFANT_NICU_ADMISSION='Y', INFANT_NICU_ADMISSION_RECODE=1),\n",
              " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0),\n",
              " Row(INFANT_NICU_ADMISSION='N', INFANT_NICU_ADMISSION_RECODE=0),\n",
              " Row(INFANT_NICU_ADMISSION='U', INFANT_NICU_ADMISSION_RECODE=0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nhQGWRUEGr2"
      },
      "source": [
        "Transform all the `YNU_cols` in one using a list of transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S3ATux-0EGr2"
      },
      "source": [
        "exprs_YNU = [\n",
        "    rec_integer(x, func.lit('YNU')).alias(x) \n",
        "    if x in YNU_cols \n",
        "    else x \n",
        "    for x in births_transformed.columns\n",
        "]\n",
        "\n",
        "births_transformed = births_transformed.select(exprs_YNU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EWoH7FeEGr3"
      },
      "source": [
        "Let's check if we got it correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-zVIKbrEGr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfebcad5-e107-473d-99bb-8530d192e7cc"
      },
      "source": [
        "births_transformed.select(YNU_cols[-5:]).show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-------------+------------+-------------+------------------+\n",
            "|DIABETES_PRE|DIABETES_GEST|HYP_TENS_PRE|HYP_TENS_GEST|PREV_BIRTH_PRETERM|\n",
            "+------------+-------------+------------+-------------+------------------+\n",
            "|           0|            0|           0|            0|                 0|\n",
            "|           0|            0|           0|            0|                 0|\n",
            "|           0|            0|           0|            0|                 0|\n",
            "|           0|            0|           0|            0|                 1|\n",
            "|           0|            0|           0|            0|                 0|\n",
            "+------------+-------------+------------+-------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4CfujwYEGr3"
      },
      "source": [
        "## Get to know your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCwVjj4EGr3"
      },
      "source": [
        "### Descriptive statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "z6PZ8fE5EGr4"
      },
      "source": [
        "ここでは、`colStats(...)`メソッドを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxAObyCLEGr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062d1355-b96c-487d-cf77-6aeb18f95916"
      },
      "source": [
        "import pyspark.mllib.stat as st\n",
        "import numpy as np\n",
        "\n",
        "numeric_cols = ['MOTHER_AGE_YEARS','FATHER_COMBINED_AGE',\n",
        "                'CIG_BEFORE','CIG_1_TRI','CIG_2_TRI','CIG_3_TRI',\n",
        "                'MOTHER_HEIGHT_IN','MOTHER_PRE_WEIGHT',\n",
        "                'MOTHER_DELIVERY_WEIGHT','MOTHER_WEIGHT_GAIN'\n",
        "               ]\n",
        "\n",
        "numeric_rdd = births_transformed\\\n",
        "                       .select(numeric_cols)\\\n",
        "                       .rdd \\\n",
        "                       .map(lambda row: [e for e in row])\n",
        "\n",
        "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
        "\n",
        "for col, m, v in zip(numeric_cols, \n",
        "                     mllib_stats.mean(), \n",
        "                     mllib_stats.variance()):\n",
        "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MOTHER_AGE_YEARS: \t28.30 \t 6.08\n",
            "FATHER_COMBINED_AGE: \t44.55 \t 27.55\n",
            "CIG_BEFORE: \t1.43 \t 5.18\n",
            "CIG_1_TRI: \t0.91 \t 3.83\n",
            "CIG_2_TRI: \t0.70 \t 3.31\n",
            "CIG_3_TRI: \t0.58 \t 3.11\n",
            "MOTHER_HEIGHT_IN: \t65.12 \t 6.45\n",
            "MOTHER_PRE_WEIGHT: \t214.50 \t 210.21\n",
            "MOTHER_DELIVERY_WEIGHT: \t223.63 \t 180.01\n",
            "MOTHER_WEIGHT_GAIN: \t30.74 \t 26.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BcYV3dzEGr4"
      },
      "source": [
        "カテゴリー変数については、その値の頻度を計算します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv9yY1fiEGr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db8226c-2534-4a71-c0a2-9cb3ab503442"
      },
      "source": [
        "categorical_cols = [e for e in births_transformed.columns \n",
        "                    if e not in numeric_cols]\n",
        "\n",
        "categorical_rdd = births_transformed\\\n",
        "                       .select(categorical_cols)\\\n",
        "                       .rdd \\\n",
        "                       .map(lambda row: [e for e in row])\n",
        "            \n",
        "for i, col in enumerate(categorical_cols):\n",
        "    agg = categorical_rdd \\\n",
        "        .groupBy(lambda row: row[i]) \\\n",
        "        .map(lambda row: (row[0], len(row[1])))\n",
        "        \n",
        "    print(col, sorted(agg.collect(), \n",
        "                      key=lambda el: el[1], \n",
        "                      reverse=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFANT_ALIVE_AT_REPORT [(1, 23349), (0, 22080)]\n",
            "BIRTH_PLACE [('1', 44558), ('4', 327), ('3', 224), ('2', 136), ('7', 91), ('5', 74), ('6', 11), ('9', 8)]\n",
            "DIABETES_PRE [(0, 44881), (1, 548)]\n",
            "DIABETES_GEST [(0, 43451), (1, 1978)]\n",
            "HYP_TENS_PRE [(0, 44348), (1, 1081)]\n",
            "HYP_TENS_GEST [(0, 43302), (1, 2127)]\n",
            "PREV_BIRTH_PRETERM [(0, 43088), (1, 2341)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d3l-rSeEGr4"
      },
      "source": [
        "### Correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouK9pMUfEGr5"
      },
      "source": [
        "Correlations between our features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ1gUrO7EGr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d2df488-715b-4a63-ca3c-3dcb55c9e195"
      },
      "source": [
        "corrs = st.Statistics.corr(numeric_rdd)\n",
        "\n",
        "for i, el in enumerate(corrs > 0.5):\n",
        "    correlated = [\n",
        "        (numeric_cols[j], corrs[i][j]) \n",
        "        for j, e in enumerate(el) \n",
        "        if e == 1.0 and j != i]\n",
        "    \n",
        "    if len(correlated) > 0:\n",
        "        for e in correlated:\n",
        "            print('{0}-to-{1}: {2:.2f}' \\\n",
        "                  .format(numeric_cols[i], e[0], e[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CIG_BEFORE-to-CIG_1_TRI: 0.83\n",
            "CIG_BEFORE-to-CIG_2_TRI: 0.72\n",
            "CIG_BEFORE-to-CIG_3_TRI: 0.62\n",
            "CIG_1_TRI-to-CIG_BEFORE: 0.83\n",
            "CIG_1_TRI-to-CIG_2_TRI: 0.87\n",
            "CIG_1_TRI-to-CIG_3_TRI: 0.76\n",
            "CIG_2_TRI-to-CIG_BEFORE: 0.72\n",
            "CIG_2_TRI-to-CIG_1_TRI: 0.87\n",
            "CIG_2_TRI-to-CIG_3_TRI: 0.89\n",
            "CIG_3_TRI-to-CIG_BEFORE: 0.62\n",
            "CIG_3_TRI-to-CIG_1_TRI: 0.76\n",
            "CIG_3_TRI-to-CIG_2_TRI: 0.89\n",
            "MOTHER_PRE_WEIGHT-to-MOTHER_DELIVERY_WEIGHT: 0.54\n",
            "MOTHER_PRE_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.65\n",
            "MOTHER_DELIVERY_WEIGHT-to-MOTHER_PRE_WEIGHT: 0.54\n",
            "MOTHER_DELIVERY_WEIGHT-to-MOTHER_WEIGHT_GAIN: 0.60\n",
            "MOTHER_WEIGHT_GAIN-to-MOTHER_PRE_WEIGHT: 0.65\n",
            "MOTHER_WEIGHT_GAIN-to-MOTHER_DELIVERY_WEIGHT: 0.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ecZvxhHaEGr5"
      },
      "source": [
        "相関性の高い特徴のほとんどを取り除く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iqN5T6WEGr5"
      },
      "source": [
        "features_to_keep = [\n",
        "    'INFANT_ALIVE_AT_REPORT', \n",
        "    'BIRTH_PLACE', \n",
        "    'MOTHER_AGE_YEARS', \n",
        "    'FATHER_COMBINED_AGE', \n",
        "    'CIG_1_TRI', \n",
        "    'MOTHER_HEIGHT_IN', \n",
        "    'MOTHER_PRE_WEIGHT', \n",
        "    'DIABETES_PRE', \n",
        "    'DIABETES_GEST', \n",
        "    'HYP_TENS_PRE', \n",
        "    'HYP_TENS_GEST', \n",
        "    'PREV_BIRTH_PRETERM'\n",
        "]\n",
        "births_transformed = births_transformed.select([e for e in features_to_keep])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGy14FQoEGr5"
      },
      "source": [
        "### Statistical testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fowhol-hEGr5"
      },
      "source": [
        "カイ二乗検定を行い、カテゴリー変数に有意な差があるかどうかを判断する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5U84JfrEGr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3cbff8-162e-45dd-a51a-41cdde534bbb"
      },
      "source": [
        "import pyspark.mllib.linalg as ln\n",
        "\n",
        "for cat in categorical_cols[1:]:\n",
        "    agg = births_transformed \\\n",
        "        .groupby('INFANT_ALIVE_AT_REPORT') \\\n",
        "        .pivot(cat) \\\n",
        "        .count()    \n",
        "\n",
        "    agg_rdd = agg \\\n",
        "        .rdd\\\n",
        "        .map(lambda row: (row[1:])) \\\n",
        "        .flatMap(lambda row: \n",
        "                 [0 if e == None else e for e in row]) \\\n",
        "        .collect()\n",
        "\n",
        "    row_length = len(agg.collect()[0]) - 1\n",
        "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
        "    \n",
        "    test = st.Statistics.chiSqTest(agg)\n",
        "    print(cat, round(test.pValue, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BIRTH_PLACE 0.0\n",
            "DIABETES_PRE 0.0\n",
            "DIABETES_GEST 0.0\n",
            "HYP_TENS_PRE 0.0\n",
            "HYP_TENS_GEST 0.0\n",
            "PREV_BIRTH_PRETERM 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWBVw4BPEGr6"
      },
      "source": [
        "## Create the final dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcmujRDBEGr6"
      },
      "source": [
        "### Create an RDD of `LabeledPoint`s\n",
        "\n",
        "`BIRTH_PLACE`をエンコードするために、ハッシュ化のトリックを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2QOT80TEGr6"
      },
      "source": [
        "import pyspark.mllib.feature as ft\n",
        "import pyspark.mllib.regression as reg\n",
        "\n",
        "hashing = ft.HashingTF(7)\n",
        "\n",
        "births_hashed = births_transformed \\\n",
        "    .rdd \\\n",
        "    .map(lambda row: [\n",
        "            list(hashing.transform(row[1]).toArray()) \n",
        "                if col == 'BIRTH_PLACE' \n",
        "                else row[i] \n",
        "            for i, col \n",
        "            in enumerate(features_to_keep)]) \\\n",
        "    .map(lambda row: [[e] if type(e) == int else e \n",
        "                      for e in row]) \\\n",
        "    .map(lambda row: [item for sublist in row \n",
        "                      for item in sublist]) \\\n",
        "    .map(lambda row: reg.LabeledPoint(\n",
        "            row[0], \n",
        "            ln.Vectors.dense(row[1:]))\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ep4mfXYEGr6"
      },
      "source": [
        "### Split into training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yHr3Zev4EGr6"
      },
      "source": [
        "モデリングの段階に移る前に、データセットをトレーニング用とテスト用の2つに分ける必要があります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AhWDogUQEGr7"
      },
      "source": [
        "births_train, births_test = births_hashed.randomSplit([0.6, 0.4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPGwtqnYmbUq",
        "outputId": "70abad8d-5068-4d7d-9f29-81b35c8a39bb"
      },
      "source": [
        "births_train.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27258"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO96-9f9megg",
        "outputId": "352d86df-b796-4abc-8073-19a8ad1ac369"
      },
      "source": [
        "births_test.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18171"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg6ehc8TEGr7"
      },
      "source": [
        "## Predicting infant survival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2dtEh5YhEGr7"
      },
      "source": [
        "### Logistic regression in Spark\n",
        "\n",
        "MLLibは、確率的勾配降下法(SGD)アルゴリズムを用いて推定したロジスティック回帰モデルを提供していました。このモデルはSpark 2.0では非推奨となっており、LogisticRegressionWithLBFGSモデルが採用されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gQ-fkfMEGr7"
      },
      "source": [
        "from pyspark.mllib.classification \\\n",
        "    import LogisticRegressionWithLBFGS\n",
        "\n",
        "LR_Model = LogisticRegressionWithLBFGS \\\n",
        "    .train(births_train, iterations=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-f4vXHBEGr7"
      },
      "source": [
        "それでは、このモデルを使って、テストセットのクラスを予測してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j0Tr9Yq3EGr7"
      },
      "source": [
        "LR_results = (\n",
        "        births_test.map(lambda row: row.label) \\\n",
        "        .zip(LR_Model \\\n",
        "             .predict(births_test\\\n",
        "                      .map(lambda row: row.features)))\n",
        "    ).map(lambda row: (row[0], row[1] * 1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kLmXPUzEGr7"
      },
      "source": [
        "それでは、私たちのモデルがどのくらいの性能を持っているのか、あるいはどのくらいの性能を持っていないのかを確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fszl8M-SEGr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0547e18-d950-487c-bd59-60b9959c4da5"
      },
      "source": [
        "import pyspark.mllib.evaluation as ev\n",
        "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
        "\n",
        "print('Area under PR: {0:.2f}' \\\n",
        "      .format(LR_evaluation.areaUnderPR))\n",
        "print('Area under ROC: {0:.2f}' \\\n",
        "      .format(LR_evaluation.areaUnderROC))\n",
        "LR_evaluation.unpersist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ac43027f6c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLR_evaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryClassificationMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLR_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Area under PR: {0:.2f}'\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLR_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mareaUnderPR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Area under ROC: {0:.2f}'\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLR_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mareaUnderROC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mLR_evaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\u001b[0m in \u001b[0;36mareaUnderPR\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mComputes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marea\u001b[0m \u001b[0munder\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0mcurve\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"areaUnderPR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1.4.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o705.areaUnderPR.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 6266, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-f151e23e4891>\", line 4, in recode\nKeyError: '5'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:192)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.pr(BinaryClassificationMetrics.scala:108)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderPR(BinaryClassificationMetrics.scala:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-f151e23e4891>\", line 4, in recode\nKeyError: '5'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNs_ECi1EGr8"
      },
      "source": [
        "### Selecting only the most predictable features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "-BdSHd7BEGr8"
      },
      "source": [
        "MLLibでは、カイ二乗セレクタを使って最も予測可能な特徴を選択することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW8_qTo7EGr8"
      },
      "source": [
        "'''\n",
        "カイ二乗セレクタを用いて\n",
        "最も予測に役立つ4つの特徴を返す\n",
        "'''\n",
        "\n",
        "selector = ft.ChiSqSelector(4).fit(births_train)\n",
        "\n",
        "topFeatures_train = (\n",
        "        births_train.map(lambda row: row.label) \\\n",
        "        .zip(selector \\\n",
        "             .transform(births_train \\\n",
        "                        .map(lambda row: row.features)))\n",
        "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))\n",
        "\n",
        "topFeatures_test = (\n",
        "        births_test.map(lambda row: row.label) \\\n",
        "        .zip(selector \\\n",
        "             .transform(births_test \\\n",
        "                        .map(lambda row: row.features)))\n",
        "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SLv3j0E7EGr8"
      },
      "source": [
        "### Random Forest in Spark\n",
        "\n",
        "これで、ランダムフォレストモデルを構築する準備が整いました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_PS_3vOEGr8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2abdab52-053c-4d12-d73a-89f087547c42"
      },
      "source": [
        "from pyspark.mllib.tree import RandomForest\n",
        "\n",
        "RF_model = RandomForest \\\n",
        "    .trainClassifier(data=topFeatures_train, \n",
        "                     numClasses=2, \n",
        "                     categoricalFeaturesInfo={}, \n",
        "                     numTrees=6,  \n",
        "                     featureSubsetStrategy='all',\n",
        "                     seed=666)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d62543ffe1ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mnumTrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      \u001b[0mfeatureSubsetStrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                      seed=666)\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36mtrainClassifier\u001b[0;34m(cls, data, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)\u001b[0m\n\u001b[1;32m    404\u001b[0m         return cls._train(data, \"classification\", numClasses,\n\u001b[1;32m    405\u001b[0m                           \u001b[0mcategoricalFeaturesInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpurity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                           maxDepth, maxBins, seed)\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/mllib/tree.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(cls, data, algo, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed)\u001b[0m\n\u001b[1;32m    303\u001b[0m     def _train(cls, data, algo, numClasses, categoricalFeaturesInfo, numTrees,\n\u001b[1;32m    304\u001b[0m                featureSubsetStrategy, impurity, maxDepth, maxBins, seed):\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"the data should be RDD of LabeledPoint\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeatureSubsetStrategy\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupportedFeatureSubsetStrategies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \"\"\"\n\u001b[0;32m-> 1393\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 6274, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-f151e23e4891>\", line 4, in recode\nKeyError: '\\x00'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 324, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 139, in dump_stream\n    for obj in iterator:\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 313, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 75, in <lambda>\n    return lambda *a: f(*a)\n  File \"/content/spark-2.3.1-bin-hadoop2.7/python/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-f151e23e4891>\", line 4, in recode\nKeyError: '\\x00'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:83)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:66)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:156)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:148)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6HQhLdMEGr8"
      },
      "source": [
        "Let's see how well our model did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qpMXWfbEGr8",
        "outputId": "d8cdd857-3f43-4c12-cf44-67bc7946250a"
      },
      "source": [
        "RF_results = (\n",
        "        topFeatures_test.map(lambda row: row.label) \\\n",
        "        .zip(RF_model \\\n",
        "             .predict(topFeatures_test \\\n",
        "                      .map(lambda row: row.features)))\n",
        "    )\n",
        "\n",
        "RF_evaluation = ev.BinaryClassificationMetrics(RF_results)\n",
        "\n",
        "print('Area under PR: {0:.2f}' \\\n",
        "      .format(RF_evaluation.areaUnderPR))\n",
        "print('Area under ROC: {0:.2f}' \\\n",
        "      .format(RF_evaluation.areaUnderROC))\n",
        "RF_evaluation.unpersist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Area under PR: 0.86\n",
            "Area under ROC: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyEoAJ1vEGr9"
      },
      "source": [
        "特徴数を減らしたロジスティック回帰の結果を見てみましょう"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x06XjOpJEGr9",
        "outputId": "0d749d9f-8051-4e25-c688-08ea50abfa95"
      },
      "source": [
        "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
        "    .train(topFeatures_train, iterations=10)\n",
        "\n",
        "LR_results_2 = (\n",
        "        topFeatures_test.map(lambda row: row.label) \\\n",
        "        .zip(LR_Model_2 \\\n",
        "             .predict(topFeatures_test \\\n",
        "                      .map(lambda row: row.features)))\n",
        "    ).map(lambda row: (row[0], row[1] * 1.0))\n",
        "\n",
        "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
        "\n",
        "print('Area under PR: {0:.2f}' \\\n",
        "      .format(LR_evaluation_2.areaUnderPR))\n",
        "print('Area under ROC: {0:.2f}' \\\n",
        "      .format(LR_evaluation_2.areaUnderROC))\n",
        "LR_evaluation_2.unpersist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Area under PR: 0.85\n",
            "Area under ROC: 0.63\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}