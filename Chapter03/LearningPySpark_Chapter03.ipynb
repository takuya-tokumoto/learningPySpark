{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningPySpark_Chapter03.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "name": "Ch4 - DataFrames",
    "notebookId": 4341522646494009
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "LRNp4sGXClqv",
        "outputId": "49f6adf5-505a-4740-b4b7-45b926794351"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "# SparkContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.getOrCreate() \n",
        "spark = SparkSession.builder.appName(\"My App\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [1 \r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,789 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [915 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 5s (2,677 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c16ebe36654f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>My App</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff8a116fad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53DLT98BkmYf",
        "outputId": "0f28eb8d-d1aa-4b8f-8c5e-63aaa977f8b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1xxHVB9CP70"
      },
      "source": [
        "## Learning PySpark\n",
        "### Chapter 4: DataFrames\n",
        "This notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMC1jlxdCP74"
      },
      "source": [
        "### Generate your own DataFrame\n",
        "ファイルシステムにアクセスするのではなく、データを生成してDataFrameを作ってみましょう。今回は、まず`stringRDD`というRDDを作成し、`spark.read.json`を使ってstringJSONRDDを読み込む際にDataFrameに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07TivN3CP75"
      },
      "source": [
        "# Generate our own JSON data \n",
        "#   This way we don't have to access the file system yet.\n",
        "stringJSONRDD = sc.parallelize((\"\"\" \n",
        "  { \"id\": \"123\",\n",
        "    \"name\": \"Katie\",\n",
        "    \"age\": 19,\n",
        "    \"eyeColor\": \"brown\"\n",
        "  }\"\"\",\n",
        "   \"\"\"{\n",
        "    \"id\": \"234\",\n",
        "    \"name\": \"Michael\",\n",
        "    \"age\": 22,\n",
        "    \"eyeColor\": \"green\"\n",
        "  }\"\"\", \n",
        "  \"\"\"{\n",
        "    \"id\": \"345\",\n",
        "    \"name\": \"Simone\",\n",
        "    \"age\": 23,\n",
        "    \"eyeColor\": \"blue\"\n",
        "  }\"\"\")\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3hjMoTPCP76"
      },
      "source": [
        "# Create DataFrame\n",
        "swimmersJSON = spark.read.json(stringJSONRDD)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSckGPCgCP77"
      },
      "source": [
        "# Create temporary table\n",
        "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1-nb15PCP77",
        "outputId": "a4d1dd7b-bac6-4384-b0dc-374e1381036b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# DataFrame API\n",
        "swimmersJSON.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------+---+-------+\n",
            "|age|eyeColor| id|   name|\n",
            "+---+--------+---+-------+\n",
            "| 19|   brown|123|  Katie|\n",
            "| 22|   green|234|Michael|\n",
            "| 23|    blue|345| Simone|\n",
            "+---+--------+---+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uRa14mlCP78",
        "outputId": "2241a39d-037e-4cc1-e3e6-16658ff98aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# SQL Query\n",
        "spark.sql(\"select * from swimmersJSON\").collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
              " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
              " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wTGeFAMpCP79",
        "outputId": "2f05e196-90da-4b31-d65e-ddaca3008bba"
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query Data\n",
        "select * from swimmersJSON\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%sql \\n-- Query Data\\nselect * from swimmersJSON\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gjwu8T9CP7-"
      },
      "source": [
        "#### Inferring the Schema Using Reflection\n",
        "リフレクションを利用してスキーマを推定する方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xDz6PvCP7_",
        "outputId": "bb0fb10c-6e46-4ddf-a37e-190a91d19258"
      },
      "source": [
        "# Print the schema\n",
        "swimmersJSON.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6VaWXZUCP8A"
      },
      "source": [
        "Sparkはスキーマを推測することができたことに注目してください（.printSchemaを使ってスキーマを確認した場合）。\n",
        "\n",
        "しかし、プログラムでスキーマを指定したい場合はどうでしょうか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meikgvvOCP8A"
      },
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "In this case, let's specify the schema for a `CSV` text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inekte2ACP8B"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Generate our own CSV data \n",
        "#   This way we don't have to access the file system yet.\n",
        "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
        "\n",
        "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
        "schemaString = \"id name age eyeColor\"\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),    \n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"eyeColor\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Apply the schema to the RDD and Create DataFrame\n",
        "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
        "\n",
        "# Creates a temporary view using the DataFrame\n",
        "swimmers.createOrReplaceTempView(\"swimmers\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhyBUtL_CP8B",
        "outputId": "6152649b-03ed-4f02-9900-6dea1dd72812"
      },
      "source": [
        "# Print the schema\n",
        "#   Notice that we have redefined id as Long (instead of String)\n",
        "swimmers.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPW3hOwKCP8C",
        "outputId": "e89c9c11-3ddb-421c-edde-e24a5b851796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query the data\n",
        "select * from swimmers\n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%sql \\n-- Query the data\\nselect * from swimmers\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jjye4gBCP8C"
      },
      "source": [
        "上記からわかるように，Sparkエンジンがリフレクションによってスキーマを推測するのではなく，プログラムによってスキーマを適用することができます。\n",
        "\n",
        "Additional Resources include:\n",
        "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
        "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiSeiDi7CP8C"
      },
      "source": [
        "####|| SparkSession\n",
        " \n",
        "`sqlContext.read...`ではなく、`spark.read....` になっていることに注目してください。これは、Spark 2.0の一部として、`HiveContext`、`SQLContext`、`StreamingContext`、`SparkContext`がSpark Session `spark`に統合されたためです。\n",
        "* データ読み込みのエントリーポイント\n",
        "* メタデータの活用\n",
        "* Configuration\n",
        "* クラスターのリソース管理\n",
        "\n",
        "For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfO8KclCP8C"
      },
      "source": [
        "### Querying with SQL\n",
        "DataFramesでは、Hive Query Language（HiveQL）と互換性のあるSQL言語である`Spark SQL`を使って、クエリを書き始めることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z88YNwGCP8D",
        "outputId": "57829ac6-f117-4da4-d7e9-ebdd95a6f38d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Execute SQL Query and return the data\n",
        "spark.sql(\"select * from swimmers\").show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC_IIw2zCP8D"
      },
      "source": [
        "Let's get the row count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lCjrFF-CP8D",
        "outputId": "97e9eb05-6218-432f-8730-e84a552e728a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get count of rows in SQL\n",
        "spark.sql(\"select count(1) from swimmers\").show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       3|\n",
            "+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA8F2JF7CP8D"
      },
      "source": [
        "なお、%sqlはDatabricksノートブックのノートブック・セル内で使用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ownGiUBkCP8E",
        "outputId": "19edc04b-2455-43b3-cd5a-5feb655e38c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query all data\n",
        "select * from swimmers\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%sql \\n-- Query all data\\nselect * from swimmers\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOpamrwCP8E",
        "outputId": "557dea0e-0a84-4483-8343-1167b2f3e74c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbHNwOfXCP8E",
        "outputId": "d22bda3f-a2b8-4273-c86a-9cef3d9acb2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
        "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ReKBj1nCP8E",
        "outputId": "cdbc2742-cfc0-4d2c-b5ce-e0e678fc545d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query id and age for swimmers with age = 22 in SQL\n",
        "spark.sql(\"select id, age from swimmers where age = 22\").show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYeUFl1MCP8F",
        "outputId": "a7bc87ec-7ca9-4a00-8371-6b39c064f2bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query id and age for swimmers with age = 22\n",
        "select id, age from swimmers where age = 22\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n%sql \\n-- Query id and age for swimmers with age = 22\\nselect id, age from swimmers where age = 22\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkJ2usSRCP8F",
        "outputId": "299637b4-ee15-4333-e535-c77ec17a94c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCqbmPVCP8F",
        "outputId": "2eff2b11-8882-4dac-c4f4-f75a0f94eb67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "select name, eyeColor from swimmers where eyeColor like 'b%\n",
        "'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n%sql \\n-- Query name and eye color for swimmers with eye color starting with the letter 'b'\\nselect name, eyeColor from swimmers where eyeColor like 'b%\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsVXQeakCP8F"
      },
      "source": [
        "### Querying with the DataFrame API\n",
        "DataFramesでは、DataFrame APIを使用してクエリを書き始めることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34CzPCeJCP8F",
        "outputId": "1741f8c0-ecce-4041-acef-11c7c668e940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show the values \n",
        "swimmers.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGyg-Y_hCP8F",
        "outputId": "019d96ce-09a9-4024-d974-8c52e148e9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Using Databricks `display` command to view the data easier\n",
        "display(swimmers)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy8MuLs1CP8G",
        "outputId": "e0950ed4-d6c7-4f0c-b06d-5b7253a096ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get count of rows\n",
        "swimmers.count()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y0dui0JCP8G",
        "outputId": "4f8a5612-98b9-43db-c489-d2a5018070c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the id, age where age = 22\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoUso_2vCP8G",
        "outputId": "efd3c846-6862-4c8d-c268-b87b3d789e5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the name, eyeColor where eyeColor like 'b%'\n",
        "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkFdB0-5CP8G"
      },
      "source": [
        "## On-Time Flight Performance\n",
        "フライトの出発遅延を州と都市別に検索するには、出発遅延を結合し、空港コード（州と都市を識別するため）に結合します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a65KuJSCP8G"
      },
      "source": [
        "### DataFrame Queries\n",
        "DataFramesを使ってフライトパフォーマンスを実行してみましょう。まず、ソースデータセットからDataFramesを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GmtyqzMCP8G",
        "outputId": "c0693754-af22-4e6a-a211-8318405d1043",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Set File Paths\n",
        "# flightPerfFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\n",
        "# airportsFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n",
        "flightPerfFilePath = \"/content/drive/MyDrive/Colab Notebooks/data/departuredelays.csv\"\n",
        "airportsFilePath = \"/content/drive/MyDrive/Colab Notebooks/data/airport-codes-na.txt\"\n",
        "\n",
        "# Obtain Airports dataset\n",
        "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
        "airports.createOrReplaceTempView(\"airports\")\n",
        "\n",
        "# Obtain Departure Delays dataset\n",
        "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
        "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
        "\n",
        "# Cache the Departure Delays dataset \n",
        "flightPerf.cache()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBN2ganTCP8H",
        "outputId": "5a737fa5-a127-4b60-a66a-0c71dd26062d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------+--------+\n",
            "|   City|origin|  Delays|\n",
            "+-------+------+--------+\n",
            "|Seattle|   SEA|159086.0|\n",
            "|Spokane|   GEG| 12404.0|\n",
            "|  Pasco|   PSC|   949.0|\n",
            "+-------+------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFITcaDaCP8H",
        "outputId": "96e8202c-1893-44c5-f676-6f86206298ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''%sql\n",
        "-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "select a.City, f.origin, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.State = 'WA'\n",
        " group by a.City, f.origin\n",
        " order by sum(f.delay) desc\n",
        " '''"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"%sql\\n-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\\nselect a.City, f.origin, sum(f.delay) as Delays\\n  from FlightPerformance f\\n    join airports a\\n      on a.IATA = f.origin\\n where a.State = 'WA'\\n group by a.City, f.origin\\n order by sum(f.delay) desc\\n \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upi9lqb5CP8H",
        "outputId": "6f48aae5-df2a-480f-f78c-c51f2d496301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Query Sum of Flight Delays by State (for the US)\n",
        "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------+\n",
            "|State|   Delays|\n",
            "+-----+---------+\n",
            "|   SC|  80666.0|\n",
            "|   AZ| 401793.0|\n",
            "|   LA| 199136.0|\n",
            "|   MN| 256811.0|\n",
            "|   NJ| 452791.0|\n",
            "|   OR| 109333.0|\n",
            "|   VA|  98016.0|\n",
            "| null| 397237.0|\n",
            "|   RI|  30760.0|\n",
            "|   WY|  15365.0|\n",
            "|   KY|  61156.0|\n",
            "|   NH|  20474.0|\n",
            "|   MI| 366486.0|\n",
            "|   NV| 474208.0|\n",
            "|   WI| 152311.0|\n",
            "|   ID|  22932.0|\n",
            "|   CA|1891919.0|\n",
            "|   CT|  54662.0|\n",
            "|   NE|  59376.0|\n",
            "|   MT|  19271.0|\n",
            "+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2GZINTECP8H",
        "outputId": "958ee5b7-64f1-4514-8878-5231670aa86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "%sql\n",
        "-- Query Sum of Flight Delays by State (for the US)\n",
        "select a.State, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.Country = 'USA'\n",
        " group by a.State '''"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n%sql\\n-- Query Sum of Flight Delays by State (for the US)\\nselect a.State, sum(f.delay) as Delays\\n  from FlightPerformance f\\n    join airports a\\n      on a.IATA = f.origin\\n where a.Country = 'USA'\\n group by a.State \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbEk8DF3CP8H",
        "outputId": "20330345-7387-4863-96b5-3be1bd1a85a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''%sql\n",
        "-- Query Sum of Flight Delays by State (for the US)\n",
        "select a.State, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.Country = 'USA'\n",
        " group by a.State '''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"%sql\\n-- Query Sum of Flight Delays by State (for the US)\\nselect a.State, sum(f.delay) as Delays\\n  from FlightPerformance f\\n    join airports a\\n      on a.IATA = f.origin\\n where a.Country = 'USA'\\n group by a.State \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79_i8Zj7CP8I"
      },
      "source": [
        "For more information, please refer to:\n",
        "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
        "* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
        "* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
      ]
    }
  ]
}