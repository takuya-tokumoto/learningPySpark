{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningPySpark_Chapter03.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "name": "Ch4 - DataFrames",
    "notebookId": 4341522646494009
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRNp4sGXClqv",
        "outputId": "d387e08a-9447-4ca8-e03e-524aabce7bfc"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "# SparkContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.getOrCreate() \n",
        "spark = SparkSession.builder.appName(\"My App\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1xxHVB9CP70"
      },
      "source": [
        "## Learning PySpark\n",
        "### Chapter 4: DataFrames\n",
        "This notebook contains sample code from Chapter 4 of [Learning PySpark]() focusing on PySpark and DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMC1jlxdCP74"
      },
      "source": [
        "### Generate your own DataFrame\n",
        "ファイルシステムにアクセスするのではなく、データを生成してDataFrameを作ってみましょう。今回は、まず`stringRDD`というRDDを作成し、`spark.read.json`を使ってstringJSONRDDを読み込む際にDataFrameに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07TivN3CP75"
      },
      "source": [
        "# Generate our own JSON data \n",
        "#   This way we don't have to access the file system yet.\n",
        "stringJSONRDD = sc.parallelize((\"\"\" \n",
        "  { \"id\": \"123\",\n",
        "    \"name\": \"Katie\",\n",
        "    \"age\": 19,\n",
        "    \"eyeColor\": \"brown\"\n",
        "  }\"\"\",\n",
        "   \"\"\"{\n",
        "    \"id\": \"234\",\n",
        "    \"name\": \"Michael\",\n",
        "    \"age\": 22,\n",
        "    \"eyeColor\": \"green\"\n",
        "  }\"\"\", \n",
        "  \"\"\"{\n",
        "    \"id\": \"345\",\n",
        "    \"name\": \"Simone\",\n",
        "    \"age\": 23,\n",
        "    \"eyeColor\": \"blue\"\n",
        "  }\"\"\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3hjMoTPCP76"
      },
      "source": [
        "# Create DataFrame\n",
        "swimmersJSON = spark.read.json(stringJSONRDD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSckGPCgCP77"
      },
      "source": [
        "# Create temporary table\n",
        "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1-nb15PCP77"
      },
      "source": [
        "# DataFrame API\n",
        "swimmersJSON.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uRa14mlCP78"
      },
      "source": [
        "# SQL Query\n",
        "spark.sql(\"select * from swimmersJSON\").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wTGeFAMpCP79",
        "outputId": "6d7252dd-5ce5-4da8-9cf1-7b4c493c0f89"
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query Data\n",
        "select * from swimmersJSON\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'%sql \\n-- Query Data\\nselect * from swimmersJSON'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gjwu8T9CP7-"
      },
      "source": [
        "#### Inferring the Schema Using Reflection\n",
        "リフレクションを利用してスキーマを推定する方法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xDz6PvCP7_",
        "outputId": "f11d94df-bc14-41d3-db6e-ef03a61dd955"
      },
      "source": [
        "# Print the schema\n",
        "swimmersJSON.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6VaWXZUCP8A"
      },
      "source": [
        "Sparkはスキーマを推測することができたことに注目してください（.printSchemaを使ってスキーマを確認した場合）。\n",
        "\n",
        "しかし、プログラムでスキーマを指定したい場合はどうでしょうか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meikgvvOCP8A"
      },
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "In this case, let's specify the schema for a `CSV` text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inekte2ACP8B"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Generate our own CSV data \n",
        "#   This way we don't have to access the file system yet.\n",
        "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
        "\n",
        "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
        "schemaString = \"id name age eyeColor\"\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),    \n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"eyeColor\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Apply the schema to the RDD and Create DataFrame\n",
        "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
        "\n",
        "# Creates a temporary view using the DataFrame\n",
        "swimmers.createOrReplaceTempView(\"swimmers\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhyBUtL_CP8B",
        "outputId": "5b8ab321-1bf4-4984-97ab-bbe66092e6e5"
      },
      "source": [
        "# Print the schema\n",
        "#   Notice that we have redefined id as Long (instead of String)\n",
        "swimmers.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPW3hOwKCP8C"
      },
      "source": [
        "%sql \n",
        "-- Query the data\n",
        "select * from swimmers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jjye4gBCP8C"
      },
      "source": [
        "上記からわかるように，Sparkエンジンがリフレクションによってスキーマを推測するのではなく，プログラムによってスキーマを適用することができます。\n",
        "\n",
        "Additional Resources include:\n",
        "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
        "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiSeiDi7CP8C"
      },
      "source": [
        "####|| SparkSession\n",
        " \n",
        "`sqlContext.read...`ではなく、`spark.read....` になっていることに注目してください。これは、Spark 2.0の一部として、`HiveContext`、`SQLContext`、`StreamingContext`、`SparkContext`がSpark Session `spark`に統合されたためです。\n",
        "* データ読み込みのエントリーポイント\n",
        "* メタデータの活用\n",
        "* Configuration\n",
        "* クラスターのリソース管理\n",
        "\n",
        "For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfO8KclCP8C"
      },
      "source": [
        "### Querying with SQL\n",
        "DataFramesでは、Hive Query Language（HiveQL）と互換性のあるSQL言語である`Spark SQL`を使って、クエリを書き始めることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z88YNwGCP8D"
      },
      "source": [
        "# Execute SQL Query and return the data\n",
        "spark.sql(\"select * from swimmers\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC_IIw2zCP8D"
      },
      "source": [
        "Let's get the row count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lCjrFF-CP8D"
      },
      "source": [
        "# Get count of rows in SQL\n",
        "spark.sql(\"select count(1) from swimmers\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA8F2JF7CP8D"
      },
      "source": [
        "なお、%sqlはDatabricksノートブックのノートブック・セル内で使用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ownGiUBkCP8E"
      },
      "source": [
        "'''\n",
        "%sql \n",
        "-- Query all data\n",
        "select * from swimmers\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsOpamrwCP8E"
      },
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbHNwOfXCP8E"
      },
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
        "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ReKBj1nCP8E"
      },
      "source": [
        "# Query id and age for swimmers with age = 22 in SQL\n",
        "spark.sql(\"select id, age from swimmers where age = 22\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYeUFl1MCP8F"
      },
      "source": [
        "%sql \n",
        "-- Query id and age for swimmers with age = 22\n",
        "select id, age from swimmers where age = 22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkJ2usSRCP8F"
      },
      "source": [
        "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCqbmPVCP8F"
      },
      "source": [
        "%sql \n",
        "-- Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "select name, eyeColor from swimmers where eyeColor like 'b%'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsVXQeakCP8F"
      },
      "source": [
        "### Querying with the DataFrame API\n",
        "DataFramesでは、DataFrame APIを使用してクエリを書き始めることができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34CzPCeJCP8F"
      },
      "source": [
        "# Show the values \n",
        "swimmers.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGyg-Y_hCP8F"
      },
      "source": [
        "# Using Databricks `display` command to view the data easier\n",
        "display(swimmers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy8MuLs1CP8G"
      },
      "source": [
        "# Get count of rows\n",
        "swimmers.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y0dui0JCP8G"
      },
      "source": [
        "# Get the id, age where age = 22\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoUso_2vCP8G"
      },
      "source": [
        "# Get the name, eyeColor where eyeColor like 'b%'\n",
        "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkFdB0-5CP8G"
      },
      "source": [
        "## On-Time Flight Performance\n",
        "フライトの出発遅延を州と都市別に検索するには、出発遅延を結合し、空港コード（州と都市を識別するため）に結合します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a65KuJSCP8G"
      },
      "source": [
        "### DataFrame Queries\n",
        "DataFramesを使ってフライトパフォーマンスを実行してみましょう。まず、ソースデータセットからDataFramesを構築します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GmtyqzMCP8G"
      },
      "source": [
        "# Set File Paths\n",
        "flightPerfFilePath = \"/databricks-datasets/flights/departuredelays.csv\"\n",
        "airportsFilePath = \"/databricks-datasets/flights/airport-codes-na.txt\"\n",
        "\n",
        "# Obtain Airports dataset\n",
        "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
        "airports.createOrReplaceTempView(\"airports\")\n",
        "\n",
        "# Obtain Departure Delays dataset\n",
        "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
        "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
        "\n",
        "# Cache the Departure Delays dataset \n",
        "flightPerf.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBN2ganTCP8H"
      },
      "source": [
        "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFITcaDaCP8H"
      },
      "source": [
        "%sql\n",
        "-- Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "select a.City, f.origin, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.State = 'WA'\n",
        " group by a.City, f.origin\n",
        " order by sum(f.delay) desc\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upi9lqb5CP8H"
      },
      "source": [
        "# Query Sum of Flight Delays by State (for the US)\n",
        "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2GZINTECP8H"
      },
      "source": [
        "%sql\n",
        "-- Query Sum of Flight Delays by State (for the US)\n",
        "select a.State, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.Country = 'USA'\n",
        " group by a.State "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbEk8DF3CP8H"
      },
      "source": [
        "%sql\n",
        "-- Query Sum of Flight Delays by State (for the US)\n",
        "select a.State, sum(f.delay) as Delays\n",
        "  from FlightPerformance f\n",
        "    join airports a\n",
        "      on a.IATA = f.origin\n",
        " where a.Country = 'USA'\n",
        " group by a.State "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79_i8Zj7CP8I"
      },
      "source": [
        "For more information, please refer to:\n",
        "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
        "* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
        "* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
      ]
    }
  ]
}